% ---------------------------------------------------------------------
% EG author guidelines plus sample file for EG publication using LaTeX2e input
% D.Fellner, v2.02, Jan 25, 2017


\title[]%
      {Conditional Variational Autoencoders for Image Based Scene Retrieval}

% for anonymous conference submission please enter your SUBMISSION ID
% instead of the author's name (and leave the affiliation blank) !!
% for final version: please provide your *own* ORCID in the brackets following \orcid; see https://orcid.org/ for more details.
\author[L.A. PÃ©rez Rey \& M. Holenderski \& Dmitri Jarnikov]
{\parbox{\textwidth}{\centering L. A. P\'{e}rez Rey$^{1}$, M. Holenderski$^{1}$
        and D. Jarnikov$^{1}$ 
%        S. Spencer$^2$\thanks{Chairman Siggraph Publications Board}
        }
        \\
% For Computer Graphics Forum: Please use the abbreviation of your first name.
{\parbox{\textwidth}{\centering $^1$ Eindhoven University of Technology
       }
}
}
% ------------------------------------------------------------------------

% if the Editors-in-Chief have given you the data, you may uncomment
% the following five lines and insert it here
%
% \volume{36}   % the volume in which the issue will be published;
% \issue{1}     % the issue number of the publication
% \pStartPage{1}      % set starting page


%-------------------------------------------------------------------------
\begin{document}
\maketitle
\section{Overview}

The proposed approach consists of image to image comparison with conditional variational autoencoders (CVAE) \cite{Kingma2014a}. The CVAE is a semi-supervised method for approximating the underlying generative model that produced a set of images and their corresponding class labels in terms of the so-called unobserved latent variables. Each of the input images is described in terms of a probability distribution over the latent variables and the classes.  \\

Our approach consists of using the probability distributions calculated by the CVAE for each image as a descriptor. The comparison between an image query and the 3D scene renders is with respect to the probability distributions obtained from the CVAE. The method consists of data pre-processing, training and retrieval described in the following subsections.

\section{Data Preprocessing}
Thirteen renders are obtained for each of the 3D scenes. Each of the 3D scenes has a predefined view when loaded into the SketchUp software. This view is saved as a 2D render together with twelve views at different angles around the scene as in \cite{Su2015}.\\

The training data set consists of the 3D scene renders together with the training images. All images are resized to a resolution of $64\times64$ and all pixel values are normalized to the interval $[0,1]$. Image augmentation is carried out by performing a horizontal flip to all images. The corresponding data space is $X = [0,1]^{64\times 64 \times 3}$ . 

\section{Training}
The CVAE consists of an encoder and a decoder neural network. The encoder network calculates from an image $x\in X$ the parameters of a probability distribution over the latent space $Z = \mathbb{R}^d$ and over the thirty class values in $Y = \{1,2,3,\ldots, 30\}$. The decoder network calculates from a latent variable $z\in Z$ and a class $y\in Y$, the parameters of a distribution over the data space $X$ .\\

The distributions for the encoder correspond to a normal distribution over $Z$ and a categorical distribution over $Y$. A normal distribution over $X$ is chosen for the decoder. The probabilistic model used corresponds to the M2 model described in the article \cite{Kingma2014a}. Both, the encoding and decoding neural networks are convolutional.\\

The CVAE is fed with batches of labelled images during training. The loss function is the sum of the negative Evidence Lower Bound (ELBO) and a classification loss. The ELBO is approximated by means of the parametrization trick described in \cite{Kingma2014a, Kingma2014} and represents the variational inference objective. The classification loss for our encoding distributions over $Y$ corresponds to the cross entropy between the probability distribution over $Y$ with respect to the input label. 

\section{Retrieval}
After training, an image $x\in X$ can be described as a conditional joint distribution over $Z\times Y$. The density $q_\phi(z|x)$ corresponds to a normal distribution and $q_\phi(y|x)$ to a categorical distribution over $Y$ where $\phi$ represents the weights of the encoder neural network. The joint density corresponds to $q_\phi(z,y|x) = q_\phi(z|x) q_\phi(y|x)$.

The similarity $D$ between an input query image $x^*\in X$ and a 3D scene in terms of its $N$ rendered images $S = \{x_r\}_{r=1}^{N}$ is given by the minimum symmetrized cross entropy $H_s$ between the query and the render probability distributions, see Figure \ref{fig:my_label}.
\begin{multline}
 D(x^*, S)\min_{r\in\{1,2,\ldots,13\} }H_s(q_\phi(z|x^*),q_\phi(z|x_{r}))\\
+\alpha H_s(q_\phi(y|x^*),q_\phi(y|x_{r})).
\end{multline}
We have used the parameter $\alpha = 64\times 64 \times 3$ to increase the importance of label matching. A ranking of 3D scenes is obtained for each query according to this similarity.
\section{Submissions}
We have sent five submissions corresponding to methods who differ only on the architecture of the encoding and decoding neural networks. These are described as follows:
\begin{enumerate}
    \item \textbf{CVAE-(1,2,3,4)}: CVAE with different CNN architectures for the encoder and decoder.
    \item \textbf{CVAE-VGG}: CVAE with features from pre-trained VGG \cite{gkallia2017keras_places365} on the Places data set \cite{Zhou2018} as part of the encoder.
    
\end{enumerate}





%%%
%%% Figure 1
%%%
\begin{figure}[htb]
  \centering
  % the following command controls the width of the embedded PS file
  % (relative to the width of the current column)
  \includegraphics[width=.8\linewidth]{comparison2.png}
  % replacing the above command with the one below will explicitly set
  % the bounding box of the PS figure to the rectangle (xl,yl),(xh,yh).
  % It will also prevent LaTeX from reading the PS file to determine
  % the bounding box (i.e., it will speed up the compilation process)
  % \includegraphics[width=.95\linewidth, bb=39 696 126 756]{sampleFig}
  %
  \parbox[t]{.9\columnwidth}{\relax
           For all figures please keep in mind that you \textbf{must not}
           use images with transparent background! 
           }
  %
  \caption{\label{fig:firstExample}
           Here is a sample figure.}
\end{figure}



%------------------------------------------------------------------------







%-------------------------------------------------------------------------

%\bibliographystyle{eg-alpha}
\bibliographystyle{eg-alpha-doi}

\bibliography{referencesmodified}

%-------------------------------------------------------------------------
\newpage


\end{document}