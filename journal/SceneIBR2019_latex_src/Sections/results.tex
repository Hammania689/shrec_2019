\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{figure}[!htp]
	\centering
	{
%		\includegraphics[width=1\textwidth,height=0.5\textheight]{result.png}
		\includegraphics[width=1.0\linewidth]{result.png}
	}
	\caption{ Precision-Recall diagram performance comparisons on testing dataset of of our \textbf{SceneIBR19} benchmark for the three learning-based participating methods.}
	\label{Results}
\end{figure}



\begin{table*}[!htb]
	\centering
	\caption{Performance metrics comparison on the SHREC'19 SceneIBR Track Benchmark.}
	\label{tab:IBR19}
	\begin{tabular}{llccccccc}
		\hline		
		\normalsize {\textbf{Participant}}	&\normalsize {\textbf{Method}} &\normalsize {\textbf{NN}}  &\normalsize {\textbf{FT}} &\normalsize {\textbf{ST}} &\normalsize {\textbf{E}} &\normalsize {\textbf{DCG}} &\normalsize {\textbf{AP}}\\
		\hline
		\textbf{\normalsize{Complete benchmark}}\\
		\hline
		\multirow{2}{*}{Triet} &RNIRAP1 &0.845    &0.620   &0.674   &0.618   &0.791  &  0.5436\\	
		\cline{2-8}
		&RNIRAP2 &\textbf{0.865}    &\textbf{0.749}   &\textbf{0.792}   &\textbf{0.745}   &\textbf{0.863} &\textbf{0.7221}\\	
		\hline  		 									
		\multirow{5}{*}{Rey} &CVAE-VGG &0.071    &0.054   &0.099   &0.055   &0.405 &0.0535\\ \cline{2-8}&CVAE1  &0.235    &0.187   &0.295   &0.189   &0.532 &0.1717\\
		\cline{2-8}
		&CVAE2 &0.272    &0.217   &0.331   &0.219   &0.560  &0.2013\\
		\cline{2-8}
		&CVAE3 &0.199    &0.154   &0.251   &0.157   &0.507 &0.1445\\
		\cline{2-8}
		&CVAE4 &0.211    &0.149   &0.246   &0.152   &0.505  &0.1424\\
		\hline							
		Yuan &VMV-VGG &0.122	 &0.458	 &0.573	 &0.452	 &0.644 &0.3899\\			
		\hline		
	\end{tabular}
\end{table*}

A comparative evaluation of the is performed on all methods. The measured retrieval performance is based on the seven metrics mentioned in Section~\ref{sec:Evaluation}: PR, NN, FT, ST, E, DCG and AP. \textbf{Fig.}~\ref{Results} and \textbf{Table}~\ref{tab:IBR19} compare the three learning-based participating methods on the testing dataset.

As can be seen in the aforementioned figure and table, Tran's RNIRAP algorithm (run 2) performs the best, followed by the baseline method VMV-VGG and the CVAE method (CVAE2). More details about the retrieval performance of each individual query of every participating method are available on the \textbf{SceneIBR2019} track homepage~\cite{SceneIBR19} . 

Firstly, during this year's track all the three methods submitted by the three participating groups are leaning-based methods, while there is no submission involving a non-learning based approach. In addition, all of three methods have employed a deep neural networks based learning approach. 

Secondly, we could further classify the submitted approaches at a finer granularity. Both RNIRAP and VMV-VGG utilize CNN models and a classification-based approach, which contribute a lot to their better accuracies. While, the CVAE-based method uses a conditional VAE generative model and resulted latent features to measure the 2D-3D similarities. 

Therefore, according to these two years' SHREC tracks (SHREC'19 and SHREC'18) on this topic, deep learning-based techniques are still the most promising and popular approach in tackling this new and challenging research direction.

In direct comparison to the results from \textbf{SceneIBR2018}, \textbf{SceneIBR2019} results do not preform as well. This is to be expected for the 10 scene categories in the \textbf{SceneIBR2018} benchmark were distinct and had few correlations. As explored by Yuan, J. et al \cite{MIPR}, the significant drop in performance can be attributed to the introduction of many correlating scene categories. 


Finally, we would like to compare the performance of SHREC'19 two related tracks on the topic of 3D scene retrieval. Similarly, this year in a parallel way we also have organized another SHREC'19 track on ``Extended 2D Sketch-Based 3D Scene Retrieval''~\cite{SceneIBR19}, based on the same target 3D scene dataset and a different query dataset which contains 25 sketches for each of the 30 classes. Except CVAE, these two tracks share other two participating methods (with minor differences). It is the second time that we have found the performance on the performance achieved in this extended ``Image-Based 3D Scene Retrieval (IBR)'' track is significantly better, compared with that achieved on the back to back extended ``Sketch-Based 3D Scene Retrieval (IBR)'' track. This should be attributed to the same reasons as we have concluded in~\cite{SceneIBR19}: IBR has a much larger query training dataset which contains images, instead of sketches, that have much more details and color information as well, which makes the semantic gap between the 2D query image and 3D target scenes much smaller.  
\end{document}