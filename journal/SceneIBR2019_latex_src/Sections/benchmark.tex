\documentclass[../main.tex]{subfiles}

\subsection{Overview}
\begin{document}
\textbf{Building process.} 
Scene categories were selected from  Places \cite{zhou2017places} and with the criteria of selection being \textit{popularity}, in terms of the degree to which they are commonly seen. Through a three-person voting mechanism we selected the most popular 30 scene classes (including the initial 10 classes in \textbf{SceneIBR2018}) from the 88 scene classes of Places88 dataset \cite{Places88}, which are shared by ImageNet \cite{ImageNet}, SUN \cite{SUN}, and Places \cite{zhou2017places}. 
Instances for the additional 20 classes, were sourced from Flickr \cite{Flickr} as well as Google Images\cite{GoogleImages} for images and downloaded via 3D Warehouse \cite{3DWarehouse} for scene models. 

\textbf{Benchmark details.} Our extended 2D scene image-based 3D scene retrieval benchmark \textbf{SceneIBR2019} expands the initial 10 classes of \textbf{SceneIBR2018} with 20 new classes totaling a more comprehensive dataset of 30 classes. \textbf{SceneIBR2019} contains a complete dataset of 30,000 2D scene images (1,000 per class) and 3,000 3D scene models (100 per class). Examples for each class are demonstrated in both \textbf{Fig. \ref{fig:1}} and \textbf{Fig. \ref{fig:2}}.

In the same manner as the \textbf{SceneIBR2018} track, we randomly pull 700 images and 70 models out from each class for training and the remaining 300 images and 30 models are used for testing, as shown in Table 1. If a method involves a learning-based approach, results for both the training and testing datasets need to be submitted. Otherwise, retrieval results based on the complete dataset are needed.


\begin{table}[h]
	\centering
	\caption{Training and testing datasets information of our \textbf{SceneIBR2019} benchmark.}
	\begin{center}
		\begin{tabular}  {|c|c|c|}
			\hline
			\textbf{\normalsize{Datasets}} & \textbf{\normalsize{Images}} & \textbf{\normalsize{Models}}\\
			\hline
			\normalsize{Training (per class)}  & 700  & 70  \\
			\hline
			\normalsize{Testing (per class)}  & 300  & 30  \\
			\hline
			\normalsize{Total (per class)}  & 1000  & 100  \\
			\hline
			\normalsize{Total (all 30 class)}  & 30,000  & 3,000  \\
			\hline
		\end{tabular}
	\end{center}
	%\caption {Training and testing datasets (per class) of our \textbf{SceneSBR} benchmark.}
	\label{table1}
\end{table}

\subsection{2D Scene Image Dataset} The 2D scene image query set is composed of 30,000 scene images (30 classes, each with 1,000 images) that are all from the Flicker and Google Image websites. One example per class is demonstrated in \textbf{Fig. \ref{fig:1}}.

\begin{figure}[!htp]
	\centering
%	\includegraphics[width=7.5cm,height=10cm]{figure1.png}
	\includegraphics[width=7.5cm, height=10cm]{figure1.pdf}
	\caption{Example 2D scene images (one example per class) in our \textbf{SceneIBR2019} benchmark.}
	\label{fig:1}
\end{figure}

\subsection{3D Scene Model Dataset} The 3D scene model dataset is built on the selected 3,000 3D scene models downloaded from 3D Warehouse. Each class has 100 3D scene models. One example per class is shown in \textbf{Fig. \ref{fig:2}}.

\begin{figure}[!htp]
	\centering
%		\includegraphics[width=7.5cm,height=10cm]{figure2.png}
	\includegraphics[width=7.5cm, height=10cm]{figure2.pdf}
	\caption{Example 3D scene models (one example per class, shown in one view) in our \textbf{SceneIBR2019} benchmark.}
	\label{fig:2}
\end{figure}

\subsection{Evaluation Method}
\label{sec:Evaluation}
To have a comprehensive evaluation of the retrieval algorithm, we employ 
seven commonly adopted performance metrics in 3D model retrieval 
community: Precision-Recall (PR) diagram, Nearest Neighbor (NN), First 
Tier (FT), Second Tier (ST), E-Measure (E), Discounted Cumulated Gain 
(DCG) and Average Precision (AP) \cite{CIVU}. We have developed the 
related code to compute these metrics $^{1}$.
\footnotetext{$^{1}$ \url{http://orca.st.usm.edu/~bli/SceneIBR2019}.}
\end{document}