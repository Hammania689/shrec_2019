\documentclass[../main.tex]{subfiles}

\begin{document}
\begin{center}
	\centering
	\includegraphics[width=\linewidth,height=4cm]{comparison2.png}
	\captionof{figure}{\label{fig:cvae} Overview of scene sampling and CVAE distribution learning.}
\end{center}

\subsubsection{Overview}
The proposed approach consists of image to image comparison with conditional 
variational autoencoders (CVAE) \cite{kingma2014semi}, as shown in \textbf{Fig.} 
\ref{fig:cvae}. The CVAE is a semi-supervised method for approximating the 
underlying generative model that produces a set of images and their 
corresponding class labels in terms of the so-called unobserved latent 
variables. Each of the input images is described in terms of a probability 
distribution over the latent variables and the classes. 

Their approach consists of using the probability distributions calculated by 
the CVAE for each image as a descriptor. The comparison between an image query 
and the 3D scene renderings is with respect to the probability distributions 
obtained from the CVAE. The method consists of data pre-processing, training 
and retrieval described in the following subsections.

\subsubsection{Data Preprocessing}
Thirteen renderings are obtained for each of the 3D scenes. Each of the 3D 
scenes has a predefined view when loaded into the SketchUp software. This view 
is saved as a 2D view together with twelve views at different angles around the 
scene as in \cite{Su2015}.
The training data set consists of the 3D scene renderings together with the 
training images. All images are resized to a resolution of $64\times64$ and all 
pixel values are normalized to the interval $[0,1]$. Image data augmentation is 
carried out by performing a horizontal flip to all images. The corresponding 
data space is $X = [0,1]^{64\times 64 \times 3}$, while the 3 represents the 
color space. 

\subsubsection{Training}
The CVAE consists of an encoder and a decoder neural network. The encoder network calculates from an image $x\in X$ the parameters of a probability distribution over the latent space $Z = \mathbb{R}^d$ and over the thirty class values in $Y = \{1,2,3,\ldots, 30\}$. The decoder network calculates from a latent variable $z\in Z$ and a class $y\in Y$, the parameters of a distribution over the data space $X$ .

The distributions for the encoder correspond to a normal distribution over $Z$ 
and a categorical distribution over $Y$. A normal distribution over $X$ is 
chosen for the decoder. The probabilistic model used corresponds to the M2 
model described in the article \cite{kingma2014semi}. Both the encoding and 
decoding neural networks are convolutional.

The CVAE is fed with batches of labeled images during training. The loss function is the sum of the negative Evidence Lower Bound (ELBO) and a classification loss. The ELBO is approximated by means of the parametrization trick described in \cite{kingma2014semi, kingma2013auto} and represents the variational inference objective. The classification loss for their encoding distributions over $Y$ corresponds to the cross entropy between the probability distribution over $Y$ with respect to the input label. 

\subsubsection{Retrieval}
After training, an image $x\in X$ can be described as a conditional joint 
distribution over $Z\times Y$. The density $q_\phi(z|x)$ corresponds to a 
normal distribution and $q_\phi(y|x)$ to a categorical distribution over $Y$, 
where $\phi$ represents the weights of the encoder neural network. The joint 
density corresponds to $q_\phi(z,y|x) = q_\phi(z|x) q_\phi(y|x)$.

The similarity $D$ between an input query image $x^*\in X$ and a 3D scene in 
terms of its $N$ rendered images $S = \{x_r\}_{r=1}^{N}$ is given by the 
minimum symmetrized cross entropy $H_s$ between the query and the rendered 
images' probability distributions (see \textbf{Fig.} \ref{fig:cvae}).

\begin{multline}
 D(x^*, S)\min_{r\in\{1,2,\ldots,13\} }H_s(q_\phi(z|x^*),q_\phi(z|x_{r}))\\
+\alpha H_s(q_\phi(y|x^*),q_\phi(y|x_{r})).
\end{multline}

They have used the parameter $\alpha = 64\times 64 \times 3$ to increase the importance of label matching. A ranking of 3D scenes is obtained for each query according to this similarity.

\subsubsection{Five Runs}
They have sent five submissions corresponding to methods who differ only on the architecture of the encoding and decoding neural networks. These are described as follows:
\begin{enumerate}
    \item \textbf{CVAE-(1,2,3,4)}: CVAE with different CNN architectures for the encoder and decoder.
    \item \textbf{CVAE-VGG}: CVAE with features from pre-trained VGG \cite{gkallia2017keras_places365} on the Places data set \cite{Places88} as part of the encoder.
    
\end{enumerate}


\end{document}