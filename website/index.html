<html>

<head>

	<!-- Latest compiled and minified CSS -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u"
	 crossorigin="anonymous">

	<!-- Optional theme -->
	<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp"
	 crossorigin="anonymous">

	<!-- Latest compiled and minified JavaScript -->
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa"
	 crossorigin="anonymous"></script>
	<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	<title>SHREC 2019 - 2D Scene Sketch-Based 3D Scene Retrieval</title>

	<link rel="stylesheet" type="text/css" href="index_files/style.css">
</head>

<body>
	<div id="wrapper">
		<div id="header">
			<div class="d-flex justify-content-center">
				<img class="img-fluid" id="usmlogo" src="index_files/USM.png" style="margin:auto;">
				<img class="img-fluid" id="msstatelogo" src="index_files/txstprimary.png">
				<img class="img-fluid" id="tu_graz" src="index_files/tu_graz.png">
			</div>
		</div>
		<ul id="menu">
			<li><a href="index.html">Home</a></li>
			<li><a href="data.html">DataSet</a></li>
			<li><a href="updates.html">Updates</a></li>
			<li><a href="results.html">Results</a></li>
			<li><a href="Evaluation.html">Evaluation</a></li>
			<li><a href="rank_list_format.html">Submission</a></li>
			<li><a href="faq.html">FAQ</a></li>
		</ul>
		<div id="content">
			<p>
			</p>
			<p>
			</p>

			<div class="t1">SHREC 2019 - Extended 2D Scene Image-Based 3D Scene Retrieval</div>

			<p>
			</p>
			<div class="t2">Call For Participation</div>

			<h2 id="objective">Objective:</h2>

			<p>In the months following our SHREC 2018 - 2D Scene Image-Based 3D Scene Retrieval
				<strong>(SceneIBR2018)</strong> track [1], we have extended the number of the scene categories from the initial
				10 classes in the <strong>SceneIBR2018</strong> benchmark to 30 classes [2], resulting in a new benchmark
				<strong>SceneIBR2019</strong> which has 30,000 scene images and 3,000 3D scene models. For that reason, we
				seek to further evaluate the performance of existing and new 2D scene image-based 3D scene
				retrieval algorithms using this extended and more comprehensive new benchmark</p>

			<h2 id="introduction">Introduction:</h2>

			<p>2D scene image-based 3D scene model retrieval is to retrieve 3D scene models given an input 2D
				scene image. It has vast related applications, including highly capable autonomous vehicles like
				the Renault SYMBIOZ [3] [4], multi-view 3D scene reconstruction, VR/AR scene content
				generation, and consumer electronics apps, among others. However, this task is far from trivial and lacks
				substantial research due to the challenges involved as well as a lack of related retrieval benchmarks.
				Consequently, existing 3D model retrieval algorithms have been limited to focus on single object retrieval. Seeing
				the benefits of advances in retrieving 3D scene models based on a scene image query makes this research direction
				useful, promising, and interesting as well.</p>

			<p>To promote this interesting yet challenging research, we organized a 2018 Eurographics Shape
				Retrieval Contest (SHREC) track [1] titled “2D Scene Image-Based 3D Scene Retrieval”, by
				building the first 2D scene image-based 3D scene retrieval benchmark <strong>SceneIBR2018</strong>, comprising
				10,000 2D scene images and 1,000 3D scene models. All the images and models are equally
				classified into 10 indoor as well as outdoor classes.</p>

			<p>However, as can be seen, <strong>SceneIBR2018</strong> contains only 10 distinct scene classes, and this is one
				of the
				reasons that all the three deep learning-based participating methods have achieved excellent performance on it.
				Considering this, after the track we have tripled the size of <strong>SceneIBR2018</strong>, resulting in an
				extended
				benchmark SceneIBR2019, which has 30,000 2D scene images and 3,000 3D scene models. Similarly, all the 2D images
				and 3D scene models are equally classified into 30 classes. We have kept the same set of 2D scene images and 3D
				scene models belonging to the initial 10 classes of <strong>SceneIBR2018</strong>. Hence, this track seeks
				participants who
				will provide new contributions to further advance 2D scene images-based 3D scene retrieval for evaluation and
				comparison, especially in terms of scalability to a larger number of scene categories, based on the new benchmark
				SceneIBR2019. Similarly, we will also provide corresponding evaluation code for computing a set of performance
				metrics similar to those used in the Query-by-Model retrieval technique.</p>

			<h2 id="benchmark">Benchmark:</h2>

			<p><strong>Building process</strong>. The first thing for the benchmark design is category selection, for which we
				have referred to several of the most popular 2D/3D scene datasets, such as Places [5] and SUN [6].
				The criteria for the category selection is popularity. Finally, we selected the most popular 30 scene
				classes (including the initial 10 classes in <strong>SceneIBR2018</strong>) from the 88 available category labels
				in
				the Places88 dataset [5], via a voting mechanism among three people (two graduate students as
				voters and a faculty mstrongber as the moderator) based on their judgments. We want to mention that
				the 88 common scenes are already shared by ImageNet [7], SUN [6], and Places [5]. Then, to
				collect data (images and models) for the additional 20 classes, we gathered from Flicker and Google
				Image for images, and downloaded SketchUp 3D scene models (originally, in “.SKP” format, but
				we provide “.OBJ” format as well after transformation) from 3D Warehouse [8].
				Benchmark details. Our extended 2D scene image-based 3D scene retrieval benchmark
				SceneIBR2019 expands the initial 10 classes of <strong>SceneIBR2018</strong> with 20 new classes totaling a
				more comprehensive dataset of 30 classes. SceneIBR2019 contains a complete dataset of 30,000
				2D scene images (1,000 per class) and 3,000 3D scene models (100 per class). Examples for each
				class are dstrongonstrated in both <strong>Fig. 1</strong> and <strong>Fig. 2</strong>.
				In the same manner as the <strong>SceneIBR2018</strong> track, we randomly pull 700 images and 70 models out
				from each class for training and the rstrongaining 300 images and 30 models are used for testing, as shown in
				<strong>Table
					1</strong>. If a method involves a learning-based approach, results for both the training and
				testing datasets need to be submitted. Otherwise, retrieval results based on the complete datasets
				are needed. </p>

			<p><strong>
					<p style="text-align: center;"> Table 1. Training and testing datasets information of our SceneIBR2019 benchmark.<p
						 style="text-align: center;">
				</strong></p>

			<p align="center">
				<img src="https://i.imgur.com/8ov980f.png" height="250px" width="auto">
			</p>

			<p><strong>2D Scene Image Dataset.</strong> The 2D scene image query set is composed of 30,000 scene images (30
				classes, each with 1,000 images) that are all from the Flicker and Google Image websites. One
				example per class is dstrongonstrated in <strong>Fig. 1.</strong></p>

			<p align="center">
				<img src="https://i.imgur.com/qFMCsJO.png" height="500px" width="auto">
			</p>

			<p><strong>
					<p style="text-align: center;"> Fig. 1 Example 2D scene images (one example per class) in our SceneIBR2019
						benchmark.</p>
				</strong></p>

			<p><strong>3D Scene Dataset.</strong> The 3D scene dataset is built on the selected 3,000 3D scene models
				downloaded from 3D Warehouse. Each class has 100 3D scene models. One example per class is
				shown in <strong>Fig. 2.</strong></p>

			<p align="center">
				<img src="https://imgur.com/rdtSti8.png" height="500px" width="auto">
			</p>

			<p><strong>
					<p style="text-align: center;">Fig. 2 Example 3D scene models (one example per class, shown in one view) in our
						SceneIBR2019 benchmark.</p>
				</strong></p>

			<h1 id="evaluationmethod">Evaluation Method:</h1>

			<p>To have a comprehensive evaluation of the retrieval algorithm, we strongploy seven commonly
				adopted performance metrics in 3D model retrieval community: Precision-Recall (PR) diagram,
				Nearest Neighbor (NN), First Tier (FT), Second Tier (ST), E-Measures (E), Discounted Cumulated
				Gain (DCG) and Average Precision (AP) [9]. We have developed the related code to compute these
				metrics and will provide thstrong to participants.</p>

			<h1 id="theproceduralaspects">The Procedural Aspects:</h1>

			<p>The complete dataset will be made available on the <strong>25<sup>th</sup> of January</strong> and the results
				will be
				due in
				six weeks after that. Every participant is expected to perform the queries and send us their retrieval
				results. We will then do the performance assessment. Participants and organizers will collaborate
				to write a joint SHREC track competition report to detail the results and evaluations. Results of the
				track will be presented by one of our organizers during the 2019 Eurographics 3DOR workshop in
				Genova, Italy.</p>

			<p>Preliminary Timeline:
				January 23 - Call for participation.</p>

			<p><strong>January 25</strong> -Distribution of the database. Participants can start the retrieval or train their
				algorithms.</p>

			<p><strong>February 8</strong> - Please register before this date.</p>

			<p><strong>March 8</strong> - Submission of the results on the test (for learning-based methods) or the complete
				(for
				non-learning based approaches) datasets and one-page description of their method(s).</p>

			<p>March 11 - Distribution of relevance judgments and evaluation scores.</p>

			<p>March 13 - Track is finished and results are ready for inclusion in a track report.</p>

			<p>March 15 - Submit the track report for review.</p>

			<p>March 25 - Reviews done, feedback and notifications given.</p>

			<p><strong>April 5</strong> - Camera-ready track paper submitted for inclusion in the proceedings.</p>

			<p><strong>May 5-6</strong> - Eurographics Workshop on 3D Object Retrieval 2019, featuring SHREC’2019.</p>


			<h2 id="organizers">Organizers:</h2>

			<p><a href="https://www.linkedin.com/in/thehameedabdul/">Hameed Abdul-Rashid</a>, University of SouthernMississippi,
				USA
				<br> <a href="https://www.linkedin.com/in/juefei-yuan-494a34176/">Juefei Yuan</a>, University of Southern
				Mississippi, USA
				<br> <a href="https://sites.google.com/site/libohomepage/homepage">Bo Li</a>, University of Southern Mississippi,
				USA
				<br> <a href="https://userweb.cs.txstate.edu/~yl12/">Yijuan Lu</a>, Texas State University, USA
				<br> <a href="https://www.tugraz.at/institute/cgv/schreck/">Tobias Schreck</a>, Graz University of Technology,
				Austria</p>

			<h1 id="references">References:</h1>

			<p>[1] Hameed Abdul-Rashid, Juefei Yuan, Bo Li, Yijuan Lu, Song Bai, Xiang Bai, Ngoc-Minh Bui, Minh N.
				Do, Trong-Le Do, Anh Duc Duong, Xinwei He, Tu-Khiem Le, Wenhui Li, Anan Liu, Xiaolong Liu, KhacTuan Nguyen,
				Vinh-Tiep Nguyen, Weizhi Nie, Van-Tu Ninh, Yuting Su, Vinh Ton-That, Minh-Triet
				Tran, Shu Xiang, Heyu Zhou, Yang Zhou, Zhichao Zhou. <a href="http://orca.st.usm.edu/~bli/SceneIBR2018/">SHREC’18
					Track: 2D Image-Based 3D
					Scene Retrieval</a>. 3DOR 2018: 37-44.</p>

			<p>[2] Juefei Yuan, Hameed Abdul-Rashid, Bo Li, Yijuan Lu. <a href="https://drive.google.com/open?id=1-EvRxQf9zS805vqBM413bjCDTZonb-E3">Sketch/Image-Based
					3D Scene Retrieval:
					Benchmark, Algorithm, Evaluation</a>. The IEEE 2nd International Conference on Multimedia Information
				Processing and Retrieval (MIPR’19). March 28-30, San Jose, CA, USA (Invited Paper), January 2019,
				Accepted (PDF).</p>

			<p>[3] Renault. <strong>Renault SYMBOIZ Concept</strong>.
				http://www.renault.co.uk/vehicles/concept-cars/symbiozconcept.html.</p>

			<p>[4] L. T. Tips. <strong>Driving a multi-million dollar autonomous car</strong>.
				http://www.youtube.com/watch?v=vlIJfV1u2hM\&amp;feature=youtu.be.</p>

			<p>[5] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. <strong>Places: A 10 million image database for
					scene recognition</strong>. IEEE Trans. Pattern Anal. Mach. Intell., 40(6):1452–1464, 2018.</p>

			<p>[6] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva. <strong>SUN database: Large-scale scene
					recognition from abbey to zoo</strong>. In CVPR, pages 3485-3492. IEEE Computer Society, 2010.</p>

			<p>[7] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li. <strong>ImageNet: A large-scale hierarchical image
					database</strong>. CVPR 2009: 248-255.</p>

			<p>[8] 3D Warehouse. https://3dwarehouse.sketchup.com/?hl=en.</p>

			<p>[9] Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias Schreck, Masaki Aono, Martin Burtscher, Qiang
				Chen, Nihad Karim Chowdhury, Bin Fang, Hongbo Fu, Takahiko Furuya, Haisheng Li, Jianzhuang Liu,
				Henry Johan, Ryuichi Kosaka, Hitoshi Koyanagi, Ryutarou Ohbuchi, Atsushi Tatsuma, Yajuan Wan,
				Chaoli Zhang, Changqing Zou. <strong>A comparison of 3D shape retrieval methods based on a large-scale benchmark
					supporting multimodal queries</strong>. Computer Vision and Image Understanding, 131:127, 2015. </p>
		</div>
		<div id="footer">If you have any question, please email to <a href="mailto:hameed.abdulrashid@usm.edu">Hameed
				Abdul</a></div>

	</div>
</body>

</html>